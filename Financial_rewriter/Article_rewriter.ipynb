{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "766002d7"
      },
      "source": [
        "# Task\n",
        "The task is to run a financial news pipeline. This pipeline will scrape financial news articles from specified sources, deduplicate and filter them for quality, and then attempt to rewrite a selection of these articles using the available models from Openrouterapi. The final output will be a summary of the pipeline's execution, including the number of articles scraped, aggregated, and rewritten, along with a sample of the rewritten content if the model loading was successful."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# INSTALLATION - RUN THIS FIRST!\n",
        "# ========================================\n",
        "\n",
        "# # Fast installation without compilation\n",
        "# !pip install beautifulsoup4 requests pandas lxml fake-useragent cloudscraper huggingface-hub --quiet\n",
        "\n",
        "# # Install pre-built llama-cpp-python (no compilation!)\n",
        "# !pip install llama-cpp-python --quiet\n",
        "\n",
        "# !pip install crewai crewai-tools beautifulsoup4 requests pandas newspaper3k lxml[html_clean] litellm -q\n",
        "\n",
        "\n",
        "!pip install beautifulsoup4 requests pandas newspaper3k lxml[html_clean] openai -q"
      ],
      "metadata": {
        "id": "ttTgj0wz_ERL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd917985-725e-492b-bd31-d19fa02a6436"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.4/7.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/7.4 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/7.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Financial News AI Pipeline - OPTIMIZED VERSION\n",
        "‚úì High-quality article filtering\n",
        "‚úì Human-like conversational rewriting\n",
        "‚úì Less jargon, more accessible\n",
        "‚úì Optimized for Google Colab\n",
        "\"\"\"\n",
        "\n",
        "# ===========================\n",
        "# IMPORTS\n",
        "# ===========================\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import feedparser\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "from newspaper import Article\n",
        "from openai import OpenAI\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "# ===========================\n",
        "# CONFIGURATION\n",
        "# ===========================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration\"\"\"\n",
        "\n",
        "    FREE_MODELS = [\n",
        "        \"mistralai/mistral-7b-instruct:free\",\n",
        "        \"meta-llama/llama-4-maverick:free\",\n",
        "        \"google/gemini-2.0-flash-exp:free\",\n",
        "        \"minimax/minimax-m2:free\",\n",
        "    ]\n",
        "\n",
        "    DEFAULT_MODEL = \"google/gemini-2.0-flash-exp:free\"  # Best for conversational writing\n",
        "\n",
        "# ===========================\n",
        "# HELPER FUNCTIONS\n",
        "# ===========================\n",
        "\n",
        "def get_headers():\n",
        "    \"\"\"Generate headers\"\"\"\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
        "    ]\n",
        "    return {\n",
        "        'User-Agent': random.choice(user_agents),\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "    }\n",
        "\n",
        "# ===========================\n",
        "# QUALITY SCORING FUNCTION\n",
        "# ===========================\n",
        "\n",
        "def calculate_quality_score(article: Dict) -> float:\n",
        "    \"\"\"\n",
        "    Calculate quality score for an article (0-100)\n",
        "    Higher score = better quality\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Content length (30 points max)\n",
        "    content_length = len(article.get('content', ''))\n",
        "    if content_length > 1000:\n",
        "        score += 30\n",
        "    elif content_length > 500:\n",
        "        score += 20\n",
        "    elif content_length > 200:\n",
        "        score += 10\n",
        "\n",
        "    # Title quality (20 points max)\n",
        "    title = article.get('title', '')\n",
        "    if title and title != 'N/A':\n",
        "        title_words = len(title.split())\n",
        "        if 8 <= title_words <= 15:  # Optimal title length\n",
        "            score += 20\n",
        "        elif 5 <= title_words <= 20:\n",
        "            score += 10\n",
        "\n",
        "    # Has numerical data (15 points) - quality financial articles have numbers\n",
        "    content = article.get('content', '')\n",
        "    if any(char.isdigit() for char in content):\n",
        "        score += 15\n",
        "\n",
        "    # Source credibility (20 points max)\n",
        "    trusted_sources = ['LiveMint', 'Moneycontrol', 'Economic Times', 'Business Standard']\n",
        "    if any(source.lower() in article.get('source', '').lower() for source in trusted_sources):\n",
        "        score += 20\n",
        "\n",
        "    # Recent date (15 points)\n",
        "    try:\n",
        "        article_date = article.get('date', '')\n",
        "        if article_date and article_date != 'N/A':\n",
        "            # Boost recent articles\n",
        "            score += 15\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return score\n",
        "\n",
        "# ===========================\n",
        "# SCRAPING FUNCTIONS\n",
        "# ===========================\n",
        "\n",
        "def scrape_nse_news(max_articles: int = 15) -> List[Dict]:\n",
        "    \"\"\"Scrape NSE news from Google News RSS feed\"\"\"\n",
        "    articles = []\n",
        "\n",
        "    try:\n",
        "        query = \"NSE India stock market\"\n",
        "        encoded_query = quote_plus(query)\n",
        "        rss_url = f\"https://news.google.com/rss/search?q={encoded_query}&hl=en-IN&gl=IN&ceid=IN:en\"\n",
        "\n",
        "        feed = feedparser.parse(rss_url)\n",
        "\n",
        "        if feed.entries:\n",
        "            for entry in feed.entries[:max_articles]:\n",
        "                # Get actual URL\n",
        "                actual_url = entry.link\n",
        "                if 'news.google.com' in actual_url:\n",
        "                    try:\n",
        "                        response = requests.head(actual_url, allow_redirects=True, timeout=5)\n",
        "                        actual_url = response.url\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # Get summary\n",
        "                summary = entry.get('summary', '')\n",
        "                if summary:\n",
        "                    summary = BeautifulSoup(summary, 'html.parser').get_text()\n",
        "\n",
        "                # Try to get full article content\n",
        "                try:\n",
        "                    article_obj = Article(actual_url)\n",
        "                    article_obj.download()\n",
        "                    article_obj.parse()\n",
        "                    if len(article_obj.text) > len(summary):\n",
        "                        summary = article_obj.text[:2000]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                articles.append({\n",
        "                    'source': 'Google News (NSE)',\n",
        "                    'title': entry.title,\n",
        "                    'content': summary or entry.title,\n",
        "                    'url': actual_url,\n",
        "                    'date': entry.get('published', datetime.now().strftime('%Y-%m-%d')),\n",
        "                })\n",
        "\n",
        "            print(f\"‚úì Google News (NSE): {len(articles)} articles\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Google News (NSE) error: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "def scrape_livemint(max_articles: int = 15) -> List[Dict]:\n",
        "    \"\"\"Scrape LiveMint with quality filtering\"\"\"\n",
        "    articles = []\n",
        "\n",
        "    try:\n",
        "        base_url = \"https://www.livemint.com/market\"\n",
        "        headers = get_headers()\n",
        "\n",
        "        response = requests.get(base_url, headers=headers, timeout=15)\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        article_links = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if any(s in href for s in ['/market/', '/companies/', '/money/', '/economy/']):\n",
        "                if href.startswith('/'):\n",
        "                    href = 'https://www.livemint.com' + href\n",
        "                if href not in article_links and href.startswith('http'):\n",
        "                    article_links.append(href)\n",
        "\n",
        "        for url in article_links[:max_articles]:\n",
        "            try:\n",
        "                article = Article(url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "\n",
        "                # Quality filter: at least 300 words\n",
        "                if len(article.text) > 300:\n",
        "                    articles.append({\n",
        "                        'source': 'LiveMint',\n",
        "                        'title': article.title or 'N/A',\n",
        "                        'content': article.text[:2000],\n",
        "                        'url': url,\n",
        "                        'date': str(article.publish_date) if article.publish_date else datetime.now().strftime('%Y-%m-%d'),\n",
        "                    })\n",
        "                time.sleep(1)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úì LiveMint: {len(articles)} articles\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå LiveMint: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "def scrape_moneycontrol(max_articles: int = 15) -> List[Dict]:\n",
        "    \"\"\"Scrape Moneycontrol with quality filtering\"\"\"\n",
        "    articles = []\n",
        "\n",
        "    try:\n",
        "        sections = [\n",
        "            'https://www.moneycontrol.com/news/business',\n",
        "            'https://www.moneycontrol.com/news/business/markets',\n",
        "            'https://www.moneycontrol.com/news/business/economy',\n",
        "        ]\n",
        "\n",
        "        all_links = []\n",
        "        for section_url in sections:\n",
        "            try:\n",
        "                response = requests.get(section_url, headers=get_headers(), timeout=15)\n",
        "                soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    href = link['href']\n",
        "                    if '/news/business/' in href and href not in all_links:\n",
        "                        if not href.startswith('http'):\n",
        "                            href = 'https://www.moneycontrol.com' + href\n",
        "                        all_links.append(href)\n",
        "                time.sleep(1)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        for url in all_links[:max_articles]:\n",
        "            try:\n",
        "                article = Article(url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "\n",
        "                # Quality filter: at least 300 words\n",
        "                if len(article.text) > 300:\n",
        "                    articles.append({\n",
        "                        'source': 'Moneycontrol',\n",
        "                        'title': article.title or 'N/A',\n",
        "                        'content': article.text[:2000],\n",
        "                        'url': url,\n",
        "                        'date': str(article.publish_date) if article.publish_date else datetime.now().strftime('%Y-%m-%d'),\n",
        "                    })\n",
        "                time.sleep(1)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úì Moneycontrol: {len(articles)} articles\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Moneycontrol: {e}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "# ===========================\n",
        "# AI PROCESSING (Optimized)\n",
        "# ===========================\n",
        "\n",
        "class AIProcessor:\n",
        "    \"\"\"Direct OpenRouter API client with quality focus\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        self.client = OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=api_key,\n",
        "        )\n",
        "        self.model = model\n",
        "\n",
        "    def call_llm(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:\n",
        "        \"\"\"Make LLM call\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                max_tokens=2500,  # Increased for better content\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LLM Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def aggregate_articles(self, articles: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Aggregate and filter articles with QUALITY SCORING\"\"\"\n",
        "\n",
        "        # Calculate quality scores\n",
        "        for article in articles:\n",
        "            article['quality_score'] = calculate_quality_score(article)\n",
        "\n",
        "        # Sort by quality score\n",
        "        articles_sorted = sorted(articles, key=lambda x: x['quality_score'], reverse=True)\n",
        "\n",
        "        print(f\"\\nüìä Quality Scores:\")\n",
        "        for i, article in enumerate(articles_sorted[:5], 1):\n",
        "            print(f\"  {i}. {article['title'][:60]}... (Score: {article['quality_score']:.0f}/100)\")\n",
        "\n",
        "        # Use LLM to deduplicate and categorize top articles\n",
        "        top_articles = articles_sorted[:20]\n",
        "        articles_text = json.dumps([{\n",
        "            'title': a['title'],\n",
        "            'source': a['source'],\n",
        "            'content': a['content'][:500]\n",
        "        } for a in top_articles], indent=2)\n",
        "\n",
        "        system_prompt = \"\"\"You are a financial content curator for Indian markets.\n",
        "Your task is to select the BEST, most unique articles.\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"From these high-quality articles, select the top 8-10 UNIQUE ones:\n",
        "\n",
        "{articles_text}\n",
        "\n",
        "Remove:\n",
        "- Duplicates (similar topics/titles)\n",
        "- Low-quality summaries\n",
        "- Generic market updates\n",
        "\n",
        "Select articles with:\n",
        "- Specific company news\n",
        "- Policy changes\n",
        "- Economic data\n",
        "- Market analysis\n",
        "\n",
        "Return ONLY a JSON array with article titles and categories.\n",
        "Format: [{{\"title\": \"...\", \"category\": \"stocks|markets|economy|banking|policy\"}}]\"\"\"\n",
        "\n",
        "        response = self.call_llm(system_prompt, user_prompt, temperature=0.3)\n",
        "\n",
        "        if response:\n",
        "            try:\n",
        "                json_start = response.find('[')\n",
        "                json_end = response.rfind(']') + 1\n",
        "                if json_start != -1 and json_end > json_start:\n",
        "                    selected = json.loads(response[json_start:json_end])\n",
        "\n",
        "                    # Match selected titles with original articles\n",
        "                    filtered = []\n",
        "                    for sel in selected:\n",
        "                        for article in top_articles:\n",
        "                            if sel['title'].lower() in article['title'].lower():\n",
        "                                article['category'] = sel.get('category', 'general')\n",
        "                                filtered.append(article)\n",
        "                                break\n",
        "\n",
        "                    print(f\"‚úì Selected {len(filtered)} high-quality unique articles\")\n",
        "                    return filtered\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Selection error: {e}, using top scored articles\")\n",
        "\n",
        "        # Fallback: return top scored\n",
        "        return articles_sorted[:8]\n",
        "\n",
        "    def rewrite_article(self, article: Dict) -> Dict:\n",
        "        \"\"\"Rewrite article in HUMAN, CONVERSATIONAL style\"\"\"\n",
        "\n",
        "        system_prompt = \"\"\"You are a conversational financial writer for everyday readers in India.\n",
        "\n",
        "Writing style:\n",
        "- Write like you're explaining to a friend over coffee\n",
        "- Use simple, everyday language\n",
        "- Avoid jargon - explain financial terms naturally\n",
        "- Use short sentences and paragraphs\n",
        "- Add context that makes sense to regular people\n",
        "- Keep it engaging and easy to understand\n",
        "- Use \"you\" and \"we\" to connect with readers\n",
        "\n",
        "Avoid:\n",
        "- Complex financial terminology without explanation\n",
        "- Long, dense paragraphs\n",
        "- Corporate speak or press release language\n",
        "- Overly formal tone\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"Rewrite this financial news in a simple, conversational way:\n",
        "\n",
        "Original Title: {article['title']}\n",
        "Content: {article['content'][:1200]}\n",
        "Source: {article['source']}\n",
        "\n",
        "Instructions:\n",
        "1. Create a catchy, simple headline (like BuzzFeed, not Wall Street Journal)\n",
        "2. Rewrite in 350-450 words\n",
        "3. Start with the most interesting fact\n",
        "4. Explain any financial terms simply\n",
        "5. Use Indian context (‚Çπ in crores/lakhs, relate to daily life)\n",
        "6. Make it feel like a conversation, not a report\n",
        "7. Keep ALL facts and numbers accurate\n",
        "\n",
        "Example tone:\n",
        "\"The stock market had a rough day today. If you've been watching your portfolio, you probably noticed...\"\n",
        "\n",
        "Output format:\n",
        "HEADLINE: [Your simple, catchy headline]\n",
        "\n",
        "CONTENT:\n",
        "[Your conversational rewrite - 350-450 words]\"\"\"\n",
        "\n",
        "        response = self.call_llm(system_prompt, user_prompt, temperature=0.8)  # Higher temp for creativity\n",
        "\n",
        "        if response:\n",
        "            try:\n",
        "                headline_marker = \"HEADLINE:\"\n",
        "                content_marker = \"CONTENT:\"\n",
        "\n",
        "                headline_start = response.find(headline_marker)\n",
        "                content_start = response.find(content_marker)\n",
        "\n",
        "                if headline_start != -1 and content_start != -1:\n",
        "                    new_headline = response[headline_start + len(headline_marker):content_start].strip()\n",
        "                    new_content = response[content_start + len(content_marker):].strip()\n",
        "\n",
        "                    article['rewritten_title'] = new_headline\n",
        "                    article['rewritten_content'] = new_content\n",
        "\n",
        "                    print(f\"‚úì Rewrote: {article['title'][:50]}...\")\n",
        "                else:\n",
        "                    article['rewritten_title'] = article['title']\n",
        "                    article['rewritten_content'] = response\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Rewrite parse error: {e}\")\n",
        "                article['rewritten_title'] = article['title']\n",
        "                article['rewritten_content'] = article['content']\n",
        "\n",
        "        return article\n",
        "\n",
        "# ===========================\n",
        "# MAIN PIPELINE\n",
        "# ===========================\n",
        "\n",
        "class FinancialNewsPipeline:\n",
        "    \"\"\"Optimized pipeline with quality focus\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.ai = AIProcessor(api_key, model)\n",
        "\n",
        "    def run_pipeline(self, max_articles_per_source: int = 15):\n",
        "        \"\"\"Execute optimized pipeline\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üöÄ FINANCIAL NEWS AI PIPELINE - OPTIMIZED\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # PHASE 1: SCRAPING (More articles for better selection)\n",
        "        print(\"\\n[PHASE 1: SCRAPING]\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        all_articles = []\n",
        "\n",
        "        print(\"\\nüì∞ Scraping Google News (NSE)...\")\n",
        "        all_articles.extend(scrape_nse_news(max_articles_per_source))\n",
        "\n",
        "        print(\"\\nüì∞ Scraping LiveMint...\")\n",
        "        all_articles.extend(scrape_livemint(max_articles_per_source))\n",
        "\n",
        "        print(\"\\nüì∞ Scraping Moneycontrol...\")\n",
        "        all_articles.extend(scrape_moneycontrol(max_articles_per_source))\n",
        "\n",
        "        print(f\"\\n‚úì Total scraped: {len(all_articles)} articles\")\n",
        "\n",
        "        # PHASE 2: QUALITY FILTERING & AGGREGATION\n",
        "        print(\"\\n[PHASE 2: QUALITY FILTERING & SELECTION]\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        filtered_articles = self.ai.aggregate_articles(all_articles)\n",
        "        print(f\"‚úì Selected {len(filtered_articles)} high-quality articles\")\n",
        "\n",
        "        # PHASE 3: HUMAN-STYLE REWRITING\n",
        "        print(\"\\n[PHASE 3: CONVERSATIONAL REWRITING]\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        rewritten_articles = []\n",
        "        for i, article in enumerate(filtered_articles[:8], 1):\n",
        "            print(f\"\\n[{i}/{min(8, len(filtered_articles))}] Rewriting...\")\n",
        "            rewritten = self.ai.rewrite_article(article)\n",
        "            rewritten_articles.append(rewritten)\n",
        "            time.sleep(2)  # Rate limiting\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ PIPELINE COMPLETED!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return rewritten_articles\n",
        "\n",
        "    def save_results(self, articles: List[Dict]):\n",
        "        \"\"\"Save results\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Save as JSON\n",
        "        json_file = f'financial_news_{timestamp}.json'\n",
        "        with open(json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n‚úì Saved JSON: {json_file}\")\n",
        "\n",
        "        # Save as readable text\n",
        "        txt_file = f'financial_news_{timestamp}.txt'\n",
        "        with open(txt_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(\"CONVERSATIONAL FINANCIAL NEWS ARTICLES\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "            for i, article in enumerate(articles, 1):\n",
        "                f.write(f\"\\n{'='*80}\\n\")\n",
        "                f.write(f\"ARTICLE {i}\\n\")\n",
        "                f.write(f\"{'='*80}\\n\\n\")\n",
        "                f.write(f\"Source: {article['source']}\\n\")\n",
        "                f.write(f\"Category: {article.get('category', 'N/A')}\\n\")\n",
        "                f.write(f\"Quality Score: {article.get('quality_score', 0):.0f}/100\\n\")\n",
        "                f.write(f\"Original: {article['title']}\\n\")\n",
        "                f.write(f\"URL: {article['url']}\\n\\n\")\n",
        "                f.write(f\"NEW HEADLINE:\\n{article.get('rewritten_title', 'N/A')}\\n\\n\")\n",
        "                f.write(f\"REWRITTEN (CONVERSATIONAL):\\n{article.get('rewritten_content', 'N/A')}\\n\\n\")\n",
        "\n",
        "        print(f\"‚úì Saved Text: {txt_file}\")\n",
        "\n",
        "        # Try Colab download\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(json_file)\n",
        "            files.download(txt_file)\n",
        "            print(\"‚úì Downloads initiated\")\n",
        "        except:\n",
        "            print(\"‚ÑπÔ∏è Files saved locally\")\n",
        "\n",
        "        # Display summary\n",
        "        print(f\"\\nüìä SUMMARY:\")\n",
        "        print(f\"High-quality articles: {len(articles)}\")\n",
        "        avg_score = sum(a.get('quality_score', 0) for a in articles) / len(articles)\n",
        "        print(f\"Average quality score: {avg_score:.1f}/100\")\n",
        "        print(f\"\\nBy category:\")\n",
        "        categories = {}\n",
        "        for a in articles:\n",
        "            cat = a.get('category', 'general')\n",
        "            categories[cat] = categories.get(cat, 0) + 1\n",
        "        for cat, count in categories.items():\n",
        "            print(f\"  - {cat}: {count}\")\n",
        "\n",
        "# ===========================\n",
        "# MAIN EXECUTION\n",
        "# ===========================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üìä FINANCIAL NEWS AI PIPELINE - OPTIMIZED\")\n",
        "    print(\"‚úì Quality-focused scraping\")\n",
        "    print(\"‚úì Human-like conversational rewriting\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nüîë Get API key: https://openrouter.ai/keys\")\n",
        "\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "    # api_key = input(\"Enter OpenRouter API key: \").strip()\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"‚ùå API key required!\")\n",
        "        return\n",
        "\n",
        "    # Model selection\n",
        "    print(\"\\nü§ñ Free Models:\")\n",
        "    for i, model in enumerate(Config.FREE_MODELS, 1):\n",
        "        print(f\"{i}. {model}\")\n",
        "\n",
        "    choice = input(f\"\\nSelect (1-{len(Config.FREE_MODELS)}, default=3 Gemini): \").strip()\n",
        "\n",
        "    try:\n",
        "        idx = int(choice) - 1 if choice else 2\n",
        "        model = Config.FREE_MODELS[idx]\n",
        "    except:\n",
        "        model = Config.DEFAULT_MODEL\n",
        "\n",
        "    print(f\"\\n‚úì Using: {model}\")\n",
        "\n",
        "    # Articles per source\n",
        "    max_articles = int(input(\"\\nArticles per source (default=15): \").strip() or \"15\")\n",
        "\n",
        "    # Run pipeline\n",
        "    pipeline = FinancialNewsPipeline(api_key, model)\n",
        "\n",
        "    try:\n",
        "        results = pipeline.run_pipeline(max_articles)\n",
        "\n",
        "        if results:\n",
        "            pipeline.save_results(results)\n",
        "            print(\"\\n‚úÖ SUCCESS! Check your conversational articles!\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è No results generated\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7hDUhc5o82GW",
        "outputId": "025f485d-5104-4337-814a-c6c6d9b834ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üìä FINANCIAL NEWS AI PIPELINE - OPTIMIZED\n",
            "‚úì Quality-focused scraping\n",
            "‚úì Human-like conversational rewriting\n",
            "================================================================================\n",
            "\n",
            "üîë Get API key: https://openrouter.ai/keys\n",
            "\n",
            "ü§ñ Free Models:\n",
            "1. mistralai/mistral-7b-instruct:free\n",
            "2. meta-llama/llama-4-maverick:free\n",
            "3. google/gemini-2.0-flash-exp:free\n",
            "4. minimax/minimax-m2:free\n",
            "\n",
            "Select (1-4, default=3 Gemini): 2\n",
            "\n",
            "‚úì Using: meta-llama/llama-4-maverick:free\n",
            "\n",
            "Articles per source (default=15): 16\n",
            "\n",
            "================================================================================\n",
            "üöÄ FINANCIAL NEWS AI PIPELINE - OPTIMIZED\n",
            "================================================================================\n",
            "\n",
            "[PHASE 1: SCRAPING]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üì∞ Scraping Google News (NSE)...\n",
            "‚úì Google News (NSE): 16 articles\n",
            "\n",
            "üì∞ Scraping LiveMint...\n",
            "‚úì LiveMint: 3 articles\n",
            "\n",
            "üì∞ Scraping Moneycontrol...\n",
            "‚úì Moneycontrol: 16 articles\n",
            "\n",
            "‚úì Total scraped: 35 articles\n",
            "\n",
            "[PHASE 2: QUALITY FILTERING & SELECTION]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìä Quality Scores:\n",
            "  1. Tata Motors Share Price LIVE: Stock falls over 3% ahead of Q... (Score: 100/100)\n",
            "  2. Business News, Economic News, Indian Stock Market News... (Score: 100/100)\n",
            "  3. Economy News - Latest News on Indian Economy, Government Pol... (Score: 100/100)\n",
            "  4. Company Business News: Latest Indian Companies News, Company... (Score: 100/100)\n",
            "  5. Mutual Funds: Debt Funds | Equity Mutual Fund Live News Upda... (Score: 100/100)\n",
            "‚úì Selected 4 high-quality unique articles\n",
            "‚úì Selected 4 high-quality articles\n",
            "\n",
            "[PHASE 3: CONVERSATIONAL REWRITING]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/4] Rewriting...\n",
            "\n",
            "[2/4] Rewriting...\n",
            "\n",
            "[3/4] Rewriting...\n",
            "‚úì Rewrote: Capillary Technologies sets IPO price band at Rs 5...\n",
            "\n",
            "[4/4] Rewriting...\n",
            "‚úì Rewrote: Defence stocks extend rally; Dynamatic, MTAR hit 5...\n",
            "\n",
            "================================================================================\n",
            "‚úÖ PIPELINE COMPLETED!\n",
            "================================================================================\n",
            "\n",
            "‚úì Saved JSON: financial_news_20251113_050659.json\n",
            "‚úì Saved Text: financial_news_20251113_050659.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bdae5051-31e2-495d-95bf-51560c05f7f9\", \"financial_news_20251113_050659.json\", 18129)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ac14035a-3c7d-49e9-a6d1-9c649421b5bc\", \"financial_news_20251113_050659.txt\", 12885)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Downloads initiated\n",
            "\n",
            "üìä SUMMARY:\n",
            "High-quality articles: 4\n",
            "Average quality score: 80.0/100\n",
            "\n",
            "By category:\n",
            "  - markets: 3\n",
            "  - stocks: 1\n",
            "\n",
            "‚úÖ SUCCESS! Check your conversational articles!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}